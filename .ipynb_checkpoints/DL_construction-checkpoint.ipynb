{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import time\n",
    "import numpy as np\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_preview_dir = './dataset/song_preview/'\n",
    "\n",
    "# importing song data\n",
    "# song_df = pandas.read_csv('./dataset/deep_learning/song_data.csv', sep='\\t')\n",
    "# genre_df = pandas.read_csv('./dataset/deep_learning/genre_song_pair.csv', sep='\\t')\n",
    "# mood_df = pandas.read_csv('./dataset/deep_learning/mood_song_pair.csv', sep='\\t')\n",
    "# tempo_df = pandas.read_csv('./dataset/deep_learning/tempo_song_pair.csv', sep='\\t')\n",
    "# song_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModelCNN(input_tensor=None, include_top=True):\n",
    "    '''Instantiate the MusicTaggerCNN architecture,\n",
    "    optionally loading weights pre-trained\n",
    "    on Million Song Dataset. Note that when using TensorFlow,\n",
    "    for best performance you should set\n",
    "    `image_dim_ordering=\"tf\"` in your Keras config\n",
    "    at ~/.keras/keras.json.\n",
    "\n",
    "    The model and the weights are compatible with both\n",
    "    TensorFlow and Theano. The dimension ordering\n",
    "    convention used by the model is the one\n",
    "    specified in your Keras config file.\n",
    "\n",
    "    For preparing mel-spectrogram input, see\n",
    "    `audio_conv_utils.py` in [applications](https://github.com/fchollet/keras/tree/master/keras/applications).\n",
    "    You will need to install [Librosa](http://librosa.github.io/librosa/)\n",
    "    to use it.\n",
    "\n",
    "    # Arguments\n",
    "        weights: one of `None` (random initialization)\n",
    "            or \"msd\" (pre-training on ImageNet).\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        include_top: whether to include the 1 fully-connected\n",
    "            layer (output layer) at the top of the network.\n",
    "            If False, the network outputs 256-dim features.\n",
    "\n",
    "\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    '''\n",
    "#     if weights not in {'msd', None}:\n",
    "#         raise ValueError('The `weights` argument should be either '\n",
    "#                          '`None` (random initialization) or `msd` '\n",
    "#                          '(pre-training on Million Song Dataset).')\n",
    "\n",
    "    K.set_image_dim_ordering('th')\n",
    "\n",
    "    # Determine proper input shape\n",
    "#     if K.image_dim_ordering() == 'th':\n",
    "    input_shape = (1, 96, 1366)\n",
    "#         # raise RuntimeError(\"th\")\n",
    "#     else:\n",
    "#         input_shape = (96, 1366, 1)\n",
    "#         # raise RuntimeError(\"tf\")\n",
    "\n",
    "#     if input_tensor is None:\n",
    "    melgram_input = Input(shape=input_shape)\n",
    "#     else:\n",
    "#         if not K.is_keras_tensor(input_tensor):\n",
    "#             melgram_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "#         else:\n",
    "#             melgram_input = input_tensor\n",
    "\n",
    "    # Determine input axis\n",
    "#     if K.image_dim_ordering() == 'th':\n",
    "    channel_axis = 1\n",
    "    freq_axis = 2\n",
    "    time_axis = 3\n",
    "#     else:\n",
    "#         channel_axis = 3\n",
    "#         freq_axis = 1\n",
    "#         time_axis = 2\n",
    "\n",
    "    # Input block\n",
    "    x = BatchNormalization(axis=freq_axis, name='bn_0_freq')(melgram_input)\n",
    "\n",
    "    # Conv block 1\n",
    "    x = Convolution2D(64, 3, 3, border_mode='same', name='conv1')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, mode=0, name='bn1')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 4), name='pool1')(x)\n",
    "\n",
    "    # Conv block 2\n",
    "    x = Convolution2D(128, 3, 3, border_mode='same', name='conv2')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, mode=0, name='bn2')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 4), name='pool2')(x)\n",
    "\n",
    "    # Conv block 3\n",
    "    x = Convolution2D(128, 3, 3, border_mode='same', name='conv3')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, mode=0, name='bn3')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 4), name='pool3')(x)\n",
    "\n",
    "    # Conv block 4\n",
    "    x = Convolution2D(128, 3, 3, border_mode='same', name='conv4')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, mode=0, name='bn4')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 5), name='pool4')(x)\n",
    "\n",
    "    # Conv block 5\n",
    "    x = Convolution2D(64, 3, 3, border_mode='same', name='conv5')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, mode=0, name='bn5')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=(4, 4), name='pool5')(x)\n",
    "\n",
    "    # Output\n",
    "    x = Flatten()(x)\n",
    "    if include_top:\n",
    "        x = Dense(50, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(melgram_input, x)\n",
    "#     if weights is None:\n",
    "    return model    \n",
    "#     else: \n",
    "#         # weights used by MSD\n",
    "#         if K.image_dim_ordering() == 'tf':\n",
    "#             raise RuntimeError(\"Please set image_dim_ordering == 'th'.\"\n",
    "#                                \"You can set it at ~/.keras/keras.json\")\n",
    "#         model.load_weights('data/music_tagger_cnn_weights_%s.h5' % K._BACKEND,\n",
    "#                            by_name=True)\n",
    "#         return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for converting audio file to melgram data\n",
    "def _compute_melgram(audio_path):\n",
    "    ''' Compute a mel-spectrogram and returns it in a shape of (1,1,96,1366), where\n",
    "    96 == #mel-bins and 1366 == #time frame\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    audio_path: path for the audio file.\n",
    "                Any format supported by audioread will work.\n",
    "    More info: http://librosa.github.io/librosa/generated/librosa.core.load.html#librosa.core.load\n",
    "\n",
    "    '''\n",
    "\n",
    "    # mel-spectrogram parameters\n",
    "    SR = 12000\n",
    "    N_FFT = 512\n",
    "    N_MELS = 96\n",
    "    HOP_LEN = 256\n",
    "    DURA = 29.12  # to make it 1366 frame..\n",
    "\n",
    "    src, sr = librosa.load(audio_path, sr=SR)  # whole signal\n",
    "    n_sample = src.shape[0]\n",
    "    n_sample_fit = int(DURA*SR)\n",
    "\n",
    "    if n_sample < n_sample_fit:  # if too short\n",
    "        src = np.hstack((src, np.zeros((int(DURA*SR) - n_sample,))))\n",
    "    elif n_sample > n_sample_fit:  # if too long\n",
    "        src = src[int((n_sample-n_sample_fit)/2):int((n_sample+n_sample_fit)/2)]\n",
    "    # logam = librosa.logamplitude\n",
    "    logam = librosa.power_to_db\n",
    "    melgram = librosa.feature.melspectrogram\n",
    "    ret = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n",
    "                        n_fft=N_FFT, n_mels=N_MELS)**2,\n",
    "                ref=1.0)\n",
    "    ret = ret[np.newaxis, np.newaxis, :]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dataset_to_array():\n",
    "    features = np.zeros((0, 1, 96, 1366))\n",
    "    \n",
    "    df = pandas.read_csv('./dataset/deep_learning/label_map(boolean_mood).csv', sep='\\t')\n",
    "    df = df.reindex(np.random.permutation(df.index)).head(1000)\n",
    "    label_matrix = df.copy()\n",
    "    label_matrix = label_matrix.drop(['track_id', 'song_id', 'title', 'preview_file'], axis=1).as_matrix()\n",
    "    for idx,row in tqdm(df.iterrows(), \"converting song audio file to features\"):\n",
    "#         print song_preview_dir+row['preview_file']\n",
    "        melgram = _compute_melgram(song_preview_dir+row['preview_file'])\n",
    "#         features.append(melgram)\n",
    "#         features.append(melgram[0])\n",
    "        features = np.concatenate((features, melgram), axis=0)\n",
    "        \n",
    "#         label = row['labels'].split(' ')\n",
    "#         labels.append(label)\n",
    "    np.save('song_features-1000.npy', features)\n",
    "    np.save('song_labels-1000.npy', label_matrix)\n",
    "    return features, label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", name=\"conv1\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:70: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(name=\"bn1\", axis=1)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:75: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", name=\"conv2\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:76: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(name=\"bn2\", axis=1)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:81: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", name=\"conv3\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(name=\"bn3\", axis=1)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:87: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", name=\"conv4\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:88: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(name=\"bn4\", axis=1)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:93: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", name=\"conv5\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:94: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(name=\"bn5\", axis=1)`\n"
     ]
    }
   ],
   "source": [
    "model = buildModelCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "split_ratio=0.8\n",
    "random_state=7\n",
    "\n",
    "# feature_path = 'song_features-1000.npy'\n",
    "# label_path = 'song_labels-1000.npy'\n",
    "# if (os.path.isfile(feature_path) and os.path.isfile(label_path)):\n",
    "#     X = np.load(feature_path)\n",
    "#     Y = np.load(label_path)\n",
    "# else:\n",
    "#     X, Y = _dataset_to_array()\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)\n",
    "# np.save('1000x_train.npy', X_train)\n",
    "# np.save('1000y_train.npy', Y_train)\n",
    "# np.save('1000x_test.npy', X_test)\n",
    "# np.save('1000y_test.npy', Y_test)\n",
    "\n",
    "if (os.path.isfile('1000x_train.npy') and os.path.isfile('1000y_train.npy') and os.path.isfile('1000x_test.npy') and os.path.isfile('1000y_test.npy')):\n",
    "    X_train = np.load('1000x_train.npy')\n",
    "    Y_train = np.load('1000y_train.npy')\n",
    "    X_test = np.load('1000x_test.npy')\n",
    "    Y_test = np.load('1000y_test.npy')\n",
    "else:\n",
    "    feature_path = 'song_features-1000.npy'\n",
    "    label_path = 'song_labels-1000.npy'\n",
    "    if (os.path.isfile(feature_path) and os.path.isfile(label_path)):\n",
    "        X = np.load(feature_path)\n",
    "        Y = np.load(label_path)\n",
    "    else:\n",
    "        X, Y = _dataset_to_array()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)\n",
    "    np.save('1000x_train.npy', X_train)\n",
    "    np.save('1000y_train.npy', Y_train)\n",
    "    np.save('1000x_test.npy', X_test)\n",
    "    np.save('1000y_test.npy', Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1, 96, 1366)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melgrams = np.zeros((0, 1, 96, 1366))\n",
    "# a = _compute_melgram('/home/capt4ce/projects/major_project/dataset/song_preview/TRAAAAW128F429D538-mzm.jmksdiul.aac.p.m4a')\n",
    "# print a.shape\n",
    "# melgrams = np.concatenate((melgrams, a), axis=0)\n",
    "# a = _compute_melgram('/home/capt4ce/projects/major_project/dataset/song_preview/TRAAABD128F429CF47-mzm.maejowgk.aac.p.m4a')\n",
    "# print a.shape\n",
    "# melgrams = np.concatenate((melgrams, a), axis=0)\n",
    "# melgrams.shape\n",
    "# print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 1727s 2s/step - loss: 14.6573 - acc: 0.0050 - val_loss: 15.8661 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 1730s 2s/step - loss: 13.4216 - acc: 0.0050 - val_loss: 14.9094 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 1734s 2s/step - loss: 13.0832 - acc: 0.0013 - val_loss: 14.8445 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 1733s 2s/step - loss: 12.8469 - acc: 0.0000e+00 - val_loss: 14.1357 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 1736s 2s/step - loss: 12.7957 - acc: 0.0038 - val_loss: 13.9867 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 1740s 2s/step - loss: 12.6150 - acc: 0.0025 - val_loss: 14.2370 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 1748s 2s/step - loss: 12.5662 - acc: 0.0188 - val_loss: 13.6271 - val_acc: 0.0250\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 1747s 2s/step - loss: 12.4285 - acc: 0.0250 - val_loss: 13.7203 - val_acc: 0.0250\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 1769s 2s/step - loss: 12.3120 - acc: 0.0413 - val_loss: 14.3133 - val_acc: 0.0300\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 1789s 2s/step - loss: 12.1820 - acc: 0.0475 - val_loss: 13.8542 - val_acc: 0.0150\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 1855s 2s/step - loss: 12.0125 - acc: 0.0563 - val_loss: 14.1269 - val_acc: 0.0350\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 1863s 2s/step - loss: 11.8588 - acc: 0.0738 - val_loss: 14.0293 - val_acc: 0.1150\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 1824s 2s/step - loss: 11.6081 - acc: 0.0838 - val_loss: 14.5425 - val_acc: 0.0700\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 1762s 2s/step - loss: 11.3906 - acc: 0.1050 - val_loss: 14.4871 - val_acc: 0.0600\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 1730s 2s/step - loss: 11.1336 - acc: 0.1250 - val_loss: 13.8981 - val_acc: 0.0200\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 1749s 2s/step - loss: 10.8812 - acc: 0.1525 - val_loss: 15.7327 - val_acc: 0.0800\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 1745s 2s/step - loss: 10.6263 - acc: 0.1425 - val_loss: 14.4204 - val_acc: 0.0850\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 1744s 2s/step - loss: 10.3060 - acc: 0.1625 - val_loss: 14.5246 - val_acc: 0.0350\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 1755s 2s/step - loss: 9.9616 - acc: 0.2075 - val_loss: 16.7144 - val_acc: 0.0300\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 1748s 2s/step - loss: 9.6796 - acc: 0.1900 - val_loss: 14.3394 - val_acc: 0.0350\n"
     ]
    }
   ],
   "source": [
    "model_path = 'DLModel.h5'\n",
    "channel = 1\n",
    "epochs = 20#50\n",
    "batch_size = 10\n",
    "verbose = 1\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(X_test, Y_test))\n",
    "# model.save(model_path)\n",
    "model.save_weights(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
